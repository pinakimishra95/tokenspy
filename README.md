# llmspy üî•

<div align="center">

**You're spending $800/month on LLMs. Which function is burning it?**

*Find out in one line. No proxy. No signup. No traffic rerouting.*

[![PyPI version](https://badge.fury.io/py/llmspy.svg)](https://badge.fury.io/py/llmspy)
[![Tests](https://github.com/pinakimishra95/llm-cost-profiler/actions/workflows/tests.yml/badge.svg)](https://github.com/pinakimishra95/llm-cost-profiler/actions)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Zero dependencies](https://img.shields.io/badge/dependencies-zero-brightgreen.svg)](https://pypi.org/project/llmspy/)

```bash
pip install llmspy
```

</div>

---

## The Problem

You get an OpenAI invoice. It says **$800 this month**. You have no idea which function caused it.

```python
def run_pipeline(query):
    docs = fetch_and_summarize(query)    # ‚Üê costs $600?
    entities = extract_entities(docs)   # ‚Üê or this one?
    return generate_report(entities)    # ‚Üê or this one?
```

Langfuse and Helicone force you to reroute traffic through their proxy. Sign up. Configure. Break your local setup.

**llmspy takes 1 line. No proxy. No signup. Runs entirely on your machine.**

---

## The Fix

```python
import llmspy

@llmspy.profile
def run_pipeline(query):
    docs = fetch_and_summarize(query)
    entities = extract_entities(docs)
    return generate_report(entities)

run_pipeline("Analyze Q3 earnings")
llmspy.report()
```

---

## Output

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  llmspy cost report                                                  ‚ïë
‚ïë  total: $0.0523  ¬∑  18,734 tokens  ¬∑  3 calls                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë                                                                      ‚ïë
‚ïë  fetch_and_summarize      $0.038  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  73%             ‚ïë
‚ïë    ‚îî‚îÄ gpt-4o               $0.038  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  73%            ‚ïë
‚ïë       ‚îî‚îÄ 12,000 tokens                                               ‚ïë
‚ïë                                                                      ‚ïë
‚ïë  generate_report          $0.011  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  21%            ‚ïë
‚ïë    ‚îî‚îÄ gpt-4o               $0.011  ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  21%            ‚ïë
‚ïë       ‚îî‚îÄ 3,600 tokens                                                ‚ïë
‚ïë                                                                      ‚ïë
‚ïë  extract_entities         $0.003  ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6%            ‚ïë
‚ïë    ‚îî‚îÄ gpt-4o-mini          $0.003  ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë   6%            ‚ïë
‚ïë       ‚îî‚îÄ 3,134 tokens                                                ‚ïë
‚ïë                                                                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  Optimization hints                                                  ‚ïë
‚ïë                                                                      ‚ïë
‚ïë  üî¥ fetch_and_summarize [gpt-4o]                                     ‚ïë
‚ïë     Switch to gpt-4o-mini ‚Äî 94% cheaper  (~$540/month savings)      ‚ïë
‚ïë                                                                      ‚ïë
‚ïë  üü° fetch_and_summarize [gpt-4o]                                     ‚ïë
‚ïë     Avg input: 12,000 tokens. Trim context or limit retrieval.       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

**Now you know: `fetch_and_summarize` is burning 73% of your budget. Fix that one function, cut your bill by $540/month.**

---

## Quick Start

### Decorator (most common)

```python
import llmspy

@llmspy.profile
def summarize_docs(docs: list[str]) -> str:
    return openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "\n".join(docs)}]
    ).choices[0].message.content

summarize_docs(my_docs)
llmspy.report()            # prints flame graph to terminal
llmspy.report("html")     # writes llmspy_report.html, opens in browser
```

### Context Manager

```python
with llmspy.session("research_task") as s:
    response = anthropic_client.messages.create(
        model="claude-haiku-4-5",
        messages=[{"role": "user", "content": query}]
    )

print(f"Cost:   {s.cost_str}")    # "$0.0012"
print(f"Tokens: {s.tokens}")      # 3,240
print(f"Calls:  {s.calls}")       # 1
```

### Programmatic Access

```python
data = llmspy.stats()
# {
#   "total_cost_usd": 0.042,
#   "total_tokens": 15000,
#   "total_calls": 3,
#   "by_function": {"summarize_docs": 0.038, "generate_report": 0.004},
#   "by_model":    {"gpt-4o": 0.040, "gpt-4o-mini": 0.002},
#   "calls": [...],
# }
```

### Persistent Tracking Across Sessions

```python
# In your app startup:
llmspy.init(persist=True)   # saves to ~/.llmspy/usage.db

# Decorate as normal ‚Äî costs accumulate across restarts
@llmspy.profile
def my_agent(query):
    ...
```

---

## How It Works

llmspy monkey-patches the SDK client **in-process** ‚Äî the same technique used by `py-spy` and `line_profiler`:

```
Your Code
    ‚îÇ
    ‚îú‚îÄ‚îÄ @llmspy.profile ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ sets active function
    ‚îÇ
    ‚îî‚îÄ‚îÄ openai_client.chat.completions.create(...)
                ‚îÇ
                ‚îî‚îÄ‚îÄ llmspy interceptor (in-process monkey-patch)
                        ‚îú‚îÄ‚îÄ calls original SDK method
                        ‚îú‚îÄ‚îÄ reads response.usage (tokens)
                        ‚îú‚îÄ‚îÄ looks up cost in built-in pricing table
                        ‚îú‚îÄ‚îÄ records: function ¬∑ model ¬∑ tokens ¬∑ cost ¬∑ duration
                        ‚îî‚îÄ‚îÄ returns response UNCHANGED to your code

llmspy.report() ‚Üí renders flame graph from recorded data
```

**No proxy server. No HTTP interception. No environment variables. No configuration.**

Your code runs exactly as before. llmspy just watches and keeps score.

---

## HTML Flame Graph

```python
llmspy.report(format="html")
```

Opens a self-contained HTML file in your browser ‚Äî zero JS dependencies, pure SVG:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  llmspy ‚Äî Total: $0.0523  (18,734 tokens)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  fetch_and_summarize  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  73%     ‚îÇ
‚îÇ  generate_report      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      21%     ‚îÇ
‚îÇ  extract_entities     ‚ñà‚ñà‚ñà‚ñà                               6%     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Model          ‚îÇ  Cost   ‚îÇ  %    ‚îÇ Input  ‚îÇ Output       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ gpt-4o         ‚îÇ $0.049  ‚îÇ  94%  ‚îÇ 15,600 ‚îÇ 4,200        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ gpt-4o-mini    ‚îÇ $0.003  ‚îÇ   6%  ‚îÇ  3,134 ‚îÇ    500       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Supported Providers

Automatically detected ‚Äî nothing to configure:

| Provider | Package | Intercepted |
|---|---|---|
| **OpenAI** | `openai>=1.0` | `chat.completions.create` (sync + async) |
| **Anthropic** | `anthropic>=0.30` | `messages.create` (sync + async) |
| **Google** | `google-generativeai>=0.7` | `generate_content` |

---

## Built-in Pricing Table

30+ models, updated Feb 2026. No API call needed.

| Model | Input $/1M | Output $/1M |
|---|---|---|
| claude-opus-4-6 | $15.00 | $75.00 |
| claude-sonnet-4-6 | $3.00 | $15.00 |
| claude-haiku-4-5 | $0.80 | $4.00 |
| gpt-4o | $2.50 | $10.00 |
| gpt-4o-mini | $0.15 | $0.60 |
| o1 | $15.00 | $60.00 |
| gemini-1.5-pro | $1.25 | $5.00 |
| gemini-1.5-flash | $0.075 | $0.30 |

[‚Üí Full pricing table](llmspy/pricing.py)

---

## API Reference

| Symbol | Description |
|---|---|
| `@llmspy.profile` | Decorator ‚Äî profile all LLM calls inside the function |
| `llmspy.session(name)` | Context manager ‚Äî profile calls in a `with` block |
| `llmspy.report()` | Print text flame graph to terminal |
| `llmspy.report(format="html")` | Write + open HTML flame graph in browser |
| `llmspy.stats()` | Return full breakdown as a dict |
| `llmspy.reset()` | Clear all recorded calls |
| `llmspy.init(persist=True)` | Enable SQLite persistence across sessions |

---

## Comparison

| | Langfuse | Helicone | LiteLLM Proxy | **llmspy** |
|---|---|---|---|---|
| Requires proxy / gateway | ‚úÖ yes | ‚úÖ yes | ‚úÖ yes | **‚ùå no** |
| Requires signup | ‚úÖ yes | ‚úÖ yes | ‚ùå no | **‚ùå no** |
| Local-first | ‚ùå no | ‚ùå no | ‚ö° partial | **‚úÖ yes** |
| Zero dependencies | ‚ùå no | ‚ùå no | ‚ùå no | **‚úÖ yes** |
| Flame graph output | ‚ùå no | ‚ùå no | ‚ùå no | **‚úÖ yes** |
| `@decorator` API | ‚ùå no | ‚ùå no | ‚ùå no | **‚úÖ yes** |
| Optimization hints | ‚ùå no | ‚ö° partial | ‚ùå no | **‚úÖ yes** |
| Works offline | ‚ùå no | ‚ùå no | ‚ö° partial | **‚úÖ yes** |

---

## Roadmap

- [ ] Streaming response support (`stream=True`)
- [ ] Token budget alerts: `@llmspy.profile(budget_usd=0.10)`
- [ ] LangChain / LangGraph integration
- [ ] CLI: `llmspy history`, `llmspy report`
- [ ] GitHub Actions annotation (cost diff per PR)
- [ ] Cost comparison across git commits

---

## Contributing

```bash
git clone https://github.com/pinakimishra95/llm-cost-profiler
cd llm-cost-profiler
pip install -e ".[dev]"
pytest tests/                # 59 tests, ~0.1s
```

Issues and PRs welcome ‚Äî especially for new provider support and updated pricing.

---

## License

MIT ¬© [Pinaki Mishra](https://github.com/pinakimishra95). See [LICENSE](LICENSE).

---

<div align="center">

**Star this repo if you're tired of mystery LLM invoices.** ‚≠ê

[GitHub](https://github.com/pinakimishra95/llm-cost-profiler) ¬∑ [PyPI](https://pypi.org/project/llmspy/) ¬∑ [Issues](https://github.com/pinakimishra95/llm-cost-profiler/issues)

</div>
